<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Gavin Gray" />
  <title>Defying the Curse of Dimensionality: Competitive Seizure Prediction with Kaggle</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="reveal.js/css/reveal.min.css" />
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Defying the Curse of Dimensionality: Competitive Seizure Prediction with Kaggle</h1>
    <h2 class="author">Gavin Gray</h2>
    <h3 class="date">November 28th 2014</h3>
</section>

<section><section id="what-does-a-kaggle-competition-look-like-to-you" class="titleslide slide level1"><h1>What does a Kaggle competition look like, to you?</h1></section><section id="the-view-from-github" class="slide level2">
<h1>The view from Github</h1>
<!---

Graphs from github showing commit logs, branches and punchcard.

--->

<aside class="notes">
<p>As many people here probably already know what a Kaggle competition is I'm going to describe what they are in a roundabout way that hopefully won't be boring to those people.</p>
<p>To get an idea of what something is, instead of describing it, what if I just show you what it looks like from some different angles? Then you can get an idea of what it is yourself. These are some ways of looking at the Kaggle competition project Scott, Finlay and I were working on.</p>
<p>These are the standard graphs github will produce if you ask it to for a given project. For example, looking at the punchcard you can see when we were making most of our commits in this project.</p>
That might say more about us than it does about the project.
</aside>
</section><section id="the-view-from-the-data" class="slide level2">
<h1>The view from the data</h1>
<!---

Graphs plotting the data, raw, processed and dimensionality reduced.

--->


<aside class="notes">
<p>The data you can see here is what we were working with. These are recordings from electrodes inside the brains of patients and dogs in two states: preictal and interictal. i.e. just before a seizure and <em>not</em> just before a seizure.</p>
There's a third colour in some of these graphs though. Those are test samples, and we're not told which type they are. From that, maybe you can guess what we were trying to do.
</aside>
</section><section id="the-view-from-kaggle" class="slide level2">
<h1>The view from Kaggle</h1>
<!---

The Kaggle leaderboard, an example ROC curve.

--->

<aside class="notes">
<p>So we try to predict whether the samples are preictal or interictal, then we supply our predictions on all of these test samples to Kaggle and they score us on the leader board.</p>
<p>That's the competition.</p>
<p>We were scored on a measure of area under the curve (AUC) on an receiver operating characteristic (ROC) curve. Basically, it's a plot of false positive versus true positive rate as the threshold of the classifier was varied.</p>
<p>We could submit ten entries a day, and receive scores on each. So you might think we could just overfit the test data by submitting a huge number of times. There's a catch, the score on the leaderboard <em>during</em> the competition is only calculated from a fraction of the submission, and at the end the real value using all of the test data is revealed.</p>
This means that at the end of the competition, there's a big shakeup of the scores. It worked pretty well for us, as we went up 15 places.
</aside>
</section></section>
<section><section id="what-can-you-work-on" class="titleslide slide level1"><h1>What can you work on?</h1></section><section id="historical-competitions" class="slide level2">
<h1>Historical competitions</h1>
<aside class="notes">
<p>Examples of historical competitions go here.</p>
</aside>
</section><section id="tools" class="slide level2">
<h1>Tools</h1>
<p>Free to use anything to get the job done. We used:</p>
<ul>
<li>Matlab</li>
<li>Scikit-learn</li>
<li>Git</li>
<li>Various other Python packages</li>
<li>Working with HDF5s</li>
<li>MongoDB</li>
</ul>
<aside class="notes">
<p>Of course, in a Kaggle competition you're free to use any tools you want to use. You could use something that you just want to learn to use, or whatever your favourite tool for the job is.</p>
Here are some of the things we used. We used Matlab for feature preprocessing as the files came in .mat format, which makes them more difficult to open than the documentation for scipy.io says.
</aside>
</section><section id="techniques" class="slide level2">
<h1>Techniques</h1>
<p>Quickly trying out various different approaches.</p>
<h3 id="preprocessing">Preprocessing</h3>
<!--- Notes from Scott's feature list page --->

<h3 id="machine-learning">Machine learning</h3>
<p>Here is a list (incomplete) of what we tried:</p>
<p>. . .</p>
<ul>
<li>Random Forests
<ul>
<li>Random forest classifiers</li>
<li>Totally random tree embedding</li>
<li>Extra-tree feature selection</li>
</ul></li>
<li>Support Vector Machines
<ul>
<li>Various different kernels</li>
</ul></li>
<li>Logistic Regression</li>
<li>Adaboost</li>
<li>Platt scaling</li>
<li>Univariate feature selection</li>
<li>Restricted Boltzmann machine</li>
<li>Recursive feature elimination</li>
<li>...</li>
</ul>
<h3 id="organising-the-project">Organising the project</h3>
<ul>
<li>Teamwork with git experience</li>
<li>TDD</li>
</ul>
<aside class="notes">
<p>Easy to use it as a opportunity to get acquainted with a new technique you might want to use in another project. You can quickly understand how to make your chosen ML algorithm work well, as you see the results right away, and it is a real problem.</p>
<p>Scott mainly worked on the preprocessing in matlab. In the repository there is a vast directory of matlab scripts, which I avoid.</p>
We were also able to put some time into learning development techniques which we wouldn't find a good excuse to look at otherwise. This was largely using unit tests in Python to get some bugs out of the code we were using to build training and test sets from the processed HDF5s.
</aside>
</section></section>
<section><section id="how-hard-are-these-competitions" class="titleslide slide level1"><h1>How hard are these competitions?</h1></section></section>
<section><section id="tips-and-tricks-in-seizure-prediction" class="titleslide slide level1"><h1>Tips and tricks in seizure prediction</h1></section><section id="our-process" class="slide level2">
<h1>Our process</h1>
<!--- Diagram of data flow --->

<aside class="notes">
<p>We did our preprocessing in Matlab and it seemed like the best way to get this data from Matlab into Python would be to write it to HDF5s for each preprocessing operation and load them into Python as required.</p>
<p>Once Scott was done with it this resulted in around 300GB of HDF5 files from around 30GB of raw data.</p>
<p>Figuring our which of these were actually going to be useful was a massive problem, considering we only had around 4000 samples spread over 7 subjects.</p>
<p>We launched several batch scripts with the hope that we might be able to find a &quot;silver bullet&quot; feature somewhere in there.</p>
This failed, and it proved extremely difficult to find anything better than our hand-picked set of features we chose at the start of classification, once the preprocessing had been finished.
</aside>
</section><section id="model-averaging" class="slide level2">
<h1>Model averaging</h1>
<!--- Obligatory picture of Daffy duck, from golden yeggs --->

<aside class="notes">
<p>So we were 10 hours from the deadline and I had completely run out of ideas. I'd been trying to improve our score all week with various different forms of feature selection and including other preprocessed features to try to improve it and had turned up nothing. Finlay and Scott had been doing the same thing, and we'd come up with nothing.</p>
<p>In a last ditch attempt to improve our score slightly I just took our two best submission csvs and averaged the predictions.</p>
<p>We immediately jumped up 4 places.</p>
Including some other high performing submissions we were able to jump up several more places before the end of the competition.
</aside>
</section><section id="windowing" class="slide level2">
<h1>Windowing</h1>
<!--- Diagram? --->


</section></section>
<section><section id="competitive-data-science" class="titleslide slide level1"><h1>Competitive Data Science</h1></section><section id="conclusions" class="slide level2">
<h1>Conclusions</h1>
<div class="fragment">
<p>Advantages:</p>
<ul>
<li>Get to try new things</li>
<li>Learn new skills</li>
<li>Working break from your PhD - you get <em>immediate feedback</em></li>
<li>Might discover something useful</li>
</ul>
</div>
<div class="fragment">
<p>Disadvantages:</p>
<ul>
<li>Can quickly absorb time</li>
<li>You have to have a good team</li>
<li>Models people create are not necessarily useful:
<ul>
<li>Netflix challenge</li>
<li>Engineered ensemble models are over-complicated</li>
</ul></li>
</ul>
</div>
</section></section>
    </div>
  </div>


  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.min.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        theme: 'solarized', // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: 'reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
